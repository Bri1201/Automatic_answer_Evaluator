{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "095fe6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "#For preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "#For count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#For synonyms\n",
    "from nltk.corpus import wordnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c26f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided by the educator\n",
    "\n",
    "## 1. Model Answer(Answer key)\n",
    "model_answer = \"\"\"ChatGPT is a large language model developed by OpenAI, \n",
    "which is designed to interact with humans using natural language\n",
    "processing. It uses a state-of-the-art artificial intelligence technique\n",
    "called the transformer model, which allows it to understand and respond to natural\n",
    "language queries with high accuracy and fluency. ChatGPT is capable of answering a\n",
    "wide range of questions on different topics, generating text, and engaging in conversations\n",
    "with users. It has been trained on a vast corpus of text data and is constantly improving\n",
    "through ongoing learning and updates. ChatGPT is widely used in \n",
    "various applications such as customer service, language translation, and chatbots.\"\"\"\n",
    "## 2. Set of Keywords\n",
    "keywords = [    'ChatGPT',    'OpenAI',    'natural language processing',\n",
    "            'topics',    'text generation',    'conversations'    'chatbots']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50dec9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provided by the student\n",
    "## 1. Example of a answer \n",
    "student_answer1 = \"\"\"OpenAI's ChatGPT is a cutting-edge language model that enables human-like interactions through natural language processing.\n",
    "By utilizing a powerful machine learning method known as the transformer model, ChatGPT can comprehend and deliver precise responses\n",
    "to a wide range of questions on diverse topics. With access to an extensive dataset of text information and \n",
    "continuous learning through updates, ChatGPT has become increasingly proficient in generating human-like text and \n",
    "engaging in conversations with users. This technology has numerous applications, including customer support, language translation, and chatbot interactions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b109eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## 2.Example of a partially correct answer\n",
    "student_answer2 = \"\"\"ChatGPT is an artificial intelligence tool that uses natural language processing \n",
    "to interact with humans. It is widely used in various applications, including customer service and chatbots, \n",
    "due to its ability to understand and respond to human queries. With the help of the transformer model, ChatGPT\n",
    "is able to generate text with high accuracy and fluency. While it is a powerful tool, it is not without its limitations\n",
    "and may not always provide the most comprehensive or accurate answers, depending on the complexity of the query. Nevertheless,\n",
    "ChatGPT remains an important technology in the field of natural language processing and is constantly evolving through\n",
    "ongoing learning\n",
    "and updates.\"\"\"\n",
    "\n",
    "\n",
    "##Why is this answer partially correct:\n",
    "# It misses the following points:\n",
    "#The size and complexity of the model: ChatGPT is one of the largest language models ever created, with billions of parameters that enable it to generate highly accurate and human-like responses.\n",
    "#The training process: ChatGPT is trained on a vast corpus of text data, which is used to fine-tune the model's parameters and improve its performance.\n",
    "#The architecture of the transformer model: The transformer model is a neural network architecture that has revolutionized the field of natural language processing by enabling more efficient and accurate language modeling.\n",
    "#The potential ethical concerns surrounding the use of AI technologies like ChatGPT, such as bias in training data or the impact of AI on employment and privacy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f657d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Example of an incorrect answer\n",
    "student_answer3 = \"\"\"ChatGPT is a type of garden tool that is used to prune trees and bushes. \n",
    "It is a handheld device that consists of a pair of sharp blades and a handle, which allows the\n",
    "user to grip and cut through branches and foliage with ease. ChatGPT is particularly useful for maintaining\n",
    "the appearance and health of trees and plants in gardens and parks. Its design is based on the principles of \n",
    "horticulture and it has been optimized for use by both professional landscapers and hobbyist gardeners. Although \n",
    "it may seem like an unusual name for a garden tool, ChatGPT has become increasingly popular in recent years due to \n",
    "its effectiveness and ease of use.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87774a68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradeAnswer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26640\\177259204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgradeAnswer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrade_answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradeAnswer'"
     ]
    }
   ],
   "source": [
    "from gradeAnswer import grade_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9598a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we pass it to any of the functions we need to pre proccess the text\n",
    "##General pipeline:\n",
    "#1. Tokenize\n",
    "#2. remove stop words\n",
    "#3. Remove punctuation\n",
    "#4. Lemmatize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ea89a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the WordNet POS tag from the Penn Treebank POS tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f884273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_keys(keywords):\n",
    "    \n",
    "    keywords = [word.lower() for word in keywords]\n",
    "     # Perform POS tagging to get the part of speech of each word\n",
    "    tagged_tokens = pos_tag(keywords)\n",
    "    \n",
    "    # Lemmatize the words based on their POS tag\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f3cf5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For preprocessing text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and punctuations\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(list(string.punctuation))\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Perform POS tagging to get the part of speech of each word\n",
    "    tagged_tokens = pos_tag(filtered_tokens)\n",
    "    \n",
    "    # Lemmatize the words based on their POS tag\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "    \n",
    "    # Join the lemmatized tokens into a single string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "197372c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def checkForKeywords(keywords, student_answer, model_answer):\n",
    "    #Preprocess keywords and texts\n",
    "    pp_keywords = preprocess_keys(keywords)\n",
    "    pp_student = preprocess_text(student_answer)\n",
    "    pp_model = preprocess_text(model_answer)\n",
    "    \n",
    "    # create CountVectorizer object and fit on document\n",
    "    vectorizer = CountVectorizer()\n",
    "    student_vec = vectorizer.fit_transform([pp_student])\n",
    "    model_vec = vectorizer.transform([pp_model])\n",
    "    \n",
    "    # transform keywords into sparse matrix\n",
    "    keyword_matrix = vectorizer.transform([''.join(pp_keywords)])\n",
    "    \n",
    "    # Compute two cosine similarities\n",
    "    sim1 = sklearn.metrics.pairwise.cosine_similarity(student_vec, keyword_matrix)[0][0]\n",
    "    sim2 = sklearn.metrics.pairwise.cosine_similarity(student_vec, model_vec)[0][0]\n",
    "    \n",
    "    return sim1, sim2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a67f6d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3312945782245396, 0.7363635349784026)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correct Answer\n",
    "checkForKeywords(keywords, student_answer1, model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "da7a4a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2222222222222222, 0.7624437362098718)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Partially correct answer\n",
    "checkForKeywords(keywords, student_answer2, model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d39dacfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.5000000000000001)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Incorrect answer\n",
    "checkForKeywords(keywords, student_answer3, model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bc9b518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We perform NER on all the nouns in the keyword list, If any of the keywords belong to PERSON, NORP, EVENT - ASSIGN MORE PRIORITY TO IT\n",
    "#make sure the profs only put vv imp proper nouns\n",
    "#If this key word is missing cut marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9c762",
   "metadata": {},
   "source": [
    "## Semantic Similarity Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "60cc5d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two texts are different and express opposite meanings.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define the two texts to compare\n",
    "text1 = \"cookies are easy to carry\"\n",
    "text2 = \"cookies are light\"\n",
    "\n",
    "# Tokenize the texts and remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens1 = [word for word in word_tokenize(text1) if not word in stop_words]\n",
    "tokens2 = [word for word in word_tokenize(text2) if not word in stop_words]\n",
    "\n",
    "# Calculate cosine similarity between the two texts\n",
    "vector1 = dict.fromkeys(set(tokens1 + tokens2), 0)\n",
    "vector2 = dict.fromkeys(set(tokens1 + tokens2), 0)\n",
    "\n",
    "for word in tokens1:\n",
    "    vector1[word] += 1\n",
    "\n",
    "for word in tokens2:\n",
    "    vector2[word] += 1\n",
    "\n",
    "dot_product = 0\n",
    "for word in vector1:\n",
    "    dot_product += vector1[word] * vector2[word]\n",
    "\n",
    "magnitude1 = 0\n",
    "for word in vector1:\n",
    "    magnitude1 += vector1[word] ** 2\n",
    "\n",
    "magnitude2 = 0\n",
    "for word in vector2:\n",
    "    magnitude2 += vector2[word] ** 2\n",
    "\n",
    "cosine_similarity = dot_product / ((magnitude1 ** 0.5) * (magnitude2 ** 0.5))\n",
    "\n",
    "# Calculate sentiment scores for the two texts\n",
    "sentiment1 = TextBlob(text1).sentiment.polarity\n",
    "sentiment2 = TextBlob(text2).sentiment.polarity\n",
    "\n",
    "# Determine if the two texts express the same or opposite meaning\n",
    "if cosine_similarity > 0.5 and sentiment1 == sentiment2:\n",
    "    print(\"The two texts are similar and express the same meaning.\")\n",
    "else:\n",
    "    print(\"The two texts are different and express opposite meanings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fdb4195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two texts are different and express opposite meanings.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the two texts to compare\n",
    "text1 = \"cookies are easy to carry\"\n",
    "text2 = \"cookies are light\"\n",
    "\n",
    "# Define the aspect to analyze\n",
    "aspect = \"cookies\"\n",
    "\n",
    "# Tokenize the texts and remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens1 = [word for word in word_tokenize(text1) if not word in stop_words]\n",
    "tokens2 = [word for word in word_tokenize(text2) if not word in stop_words]\n",
    "\n",
    "# Calculate cosine similarity between the two texts\n",
    "vector1 = dict.fromkeys(set(tokens1 + tokens2), 0)\n",
    "vector2 = dict.fromkeys(set(tokens1 + tokens2), 0)\n",
    "\n",
    "for word in tokens1:\n",
    "    vector1[word] += 1\n",
    "\n",
    "for word in tokens2:\n",
    "    vector2[word] += 1\n",
    "\n",
    "dot_product = 0\n",
    "for word in vector1:\n",
    "    dot_product += vector1[word] * vector2[word]\n",
    "\n",
    "magnitude1 = 0\n",
    "for word in vector1:\n",
    "    magnitude1 += vector1[word] ** 2\n",
    "\n",
    "magnitude2 = 0\n",
    "for word in vector2:\n",
    "    magnitude2 += vector2[word] ** 2\n",
    "\n",
    "cosine_similarity = dot_product / ((magnitude1 ** 0.5) * (magnitude2 ** 0.5))\n",
    "\n",
    "# Analyze aspect-based sentiment for the two texts\n",
    "aspect_sentiment1 = TextBlob(aspect + \" \" + text1).sentiment.polarity\n",
    "aspect_sentiment2 = TextBlob(aspect + \" \" + text2).sentiment.polarity\n",
    "\n",
    "# Determine if the two texts express the same or opposite meaning\n",
    "if cosine_similarity > 0.5 and aspect_sentiment1 * aspect_sentiment2 > 0:\n",
    "    print(\"The two texts are similar and express the same meaning.\")\n",
    "else:\n",
    "    print(\"The two texts are different and express opposite meanings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2573cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c96f684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"textattack/bert-base-uncased-rotten-tomatoes\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "8af82858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanticA(text1, text2, aspect):\n",
    "    # Tokenize the texts and aspect\n",
    "    tokens1 = tokenizer(aspect + \" \" + text1, return_tensors='pt', padding=True, truncation=True)\n",
    "    tokens2 = tokenizer(aspect + \" \" + text2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Perform semantic analysis on the texts and aspect\n",
    "    with torch.no_grad():\n",
    "        output1 = model(**tokens1)[0]\n",
    "        semantic1 = torch.softmax(output1, dim=1)[0][1].item()\n",
    "\n",
    "        output2 = model(**tokens2)[0]\n",
    "        semantic2 = torch.softmax(output2, dim=1)[0][1].item()\n",
    "\n",
    "    # Determine if the two texts express the same or opposite meaning\n",
    "    if abs(semantic1 - semantic2) < 0.1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "#     print(semantic1)\n",
    "#     print(semantic2)\n",
    "#     print(semantic1-semantic2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8f0e050a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semanticA(\"I love the cookies\", \"The cookies are beautiful\", \"cookies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "a95e15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().lower())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "944d56b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faineant', 'indolent', 'lazy', 'otiose', 'slothful', 'work-shy'}"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms(\"lazy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a0753f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantSentences(text, word):\n",
    "\n",
    "    # get synonyms of the word\n",
    "    synonyms = get_synonyms(word)\n",
    "\n",
    "    # tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # list to store sentences containing the word or its synonym\n",
    "    matched_sentences = []\n",
    "\n",
    "    # loop through each sentence\n",
    "    for sentence in sentences:\n",
    "        # tokenize the sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # check if the word or any of its synonyms are present in the sentence\n",
    "        if word in words:\n",
    "            matched_sentences.append(sentence)\n",
    "        else:\n",
    "            for synonym in synonyms:\n",
    "                if synonym in words:\n",
    "                    # replace the synonym with the word\n",
    "                    sentence = sentence.replace(synonym, word)\n",
    "                    matched_sentences.append(sentence)\n",
    "                    break\n",
    "\n",
    "    # Return the matched sentences\n",
    "    return matched_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "dd78c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sentences(sentences1, sentences2, word):\n",
    "    similarity_score = 0\n",
    "    for s1 in sentences1:\n",
    "        for s2 in sentences2:\n",
    "            if word in s1 and word in s2:\n",
    "                similarity_score += semanticA(s1, s2, word)\n",
    "            else:\n",
    "                synonyms = get_synonyms(word)\n",
    "                for syn in synonyms:\n",
    "                    if syn in s1 and syn in s2:\n",
    "                        s1_new = s1.replace(syn, word)\n",
    "                        s2_new = s2.replace(syn, word)\n",
    "                        similarity_score += semanticA(s1_new, s2_new, word)\n",
    "                        break\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "4225b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareTexts(text1, text2, keywords):\n",
    "    pp_text1 = preprocess_text(text1)\n",
    "    pp_text2 = preprocess_text(text2)\n",
    "    pp_keywords = preprocess_keys(keywords)\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    for keyword in pp_keywords:\n",
    "        list1 = getRelevantSentences(pp_text1, keyword)\n",
    "        list2 = getRelevantSentences(pp_text2, keyword)\n",
    "        \n",
    "        score = compare_sentences(list1, list2, keyword)\n",
    "        #print(keyword)\n",
    "        #print(score)\n",
    "        totalScore += score\n",
    "        #print(totalScore)\n",
    "    \n",
    "    return totalScore/len(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "1bb720d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(compareTexts(model_answer, student_answer1, keywords))\n",
    "print(compareTexts(model_answer, student_answer2, keywords))\n",
    "print(compareTexts(model_answer, student_answer3, keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fe222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
